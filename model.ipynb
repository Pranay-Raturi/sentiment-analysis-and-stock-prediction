{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Stock Prediction Model\n",
    "\n",
    "This project made by Divij Jasuja and Pranay Raturi for Data Mining Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import math\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "ERROR: Could not find a version that satisfies the requirement Transformer (from versions: none)\n",
      "ERROR: No matching distribution found for Transformer\n"
     ]
    }
   ],
   "source": [
    "%pip install Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"stocks are ok\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "scores.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Time_Created</th>\n",
       "      <th>Text</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1744146496243822931</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>@SelfMadeMastery Best: $NVDA, $CRWD, $META, $T...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>best nvda crwd meta tsla bad enph use oppurtun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1744146280576926128</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>Most Notable #Earnings Week of JAN 8th\\n\\nâ—¦ Mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>notable earnings week jan mon accd tues tlry a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1744145386850422822</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>ðŸ‡ºðŸ‡¸ U.S. ECONOMIC DATA 2ND WEEK\\n\\nTHURS.\\nâ—¦ U....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>flag united state economic data week thurs cpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1744145096592007411</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>$MARA We nailed this play. I kept on hammering...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>mara nail play kept hammer risk reward hence l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1743200305033175350</td>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>$MARA Scaled out of this position yesterday\\n\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>mara scale position yesterday last time specul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             Tweet_ID Time_Created  \\\n",
       "0           0  1744146496243822931   2024-01-07   \n",
       "1           1  1744146280576926128   2024-01-07   \n",
       "2           2  1744145386850422822   2024-01-07   \n",
       "3           3  1744145096592007411   2024-01-07   \n",
       "4           4  1743200305033175350   2024-01-05   \n",
       "\n",
       "                                                Text  Likes  Retweets  \\\n",
       "0  @SelfMadeMastery Best: $NVDA, $CRWD, $META, $T...      1         0   \n",
       "1  Most Notable #Earnings Week of JAN 8th\\n\\nâ—¦ Mo...      0         0   \n",
       "2  ðŸ‡ºðŸ‡¸ U.S. ECONOMIC DATA 2ND WEEK\\n\\nTHURS.\\nâ—¦ U....      0         0   \n",
       "3  $MARA We nailed this play. I kept on hammering...      3         0   \n",
       "4  $MARA Scaled out of this position yesterday\\n\\...      1         0   \n",
       "\n",
       "                                             cleaned  \n",
       "0  best nvda crwd meta tsla bad enph use oppurtun...  \n",
       "1  notable earnings week jan mon accd tues tlry a...  \n",
       "2  flag united state economic data week thurs cpi...  \n",
       "3  mara nail play kept hammer risk reward hence l...  \n",
       "4  mara scale position yesterday last time specul...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tweets = pd.read_csv('twitter_data/NVDA_final-tweets')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scentiment_analysis(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True, max_length=512)\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return scores.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
=======
      "Collecting accelerate\n",
      "  Downloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (2.0.1)\n",
      "Collecting huggingface-hub (from accelerate)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting safetensors>=0.3.1 (from accelerate)\n",
      "  Downloading safetensors-0.4.3-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub->accelerate)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\divij\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python311\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
      "   ---------------------------------------- 0.0/297.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/297.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/297.6 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 61.4/297.6 kB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 297.6/297.6 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp311-none-win_amd64.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.3 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 163.8/287.3 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 163.8/287.3 kB 9.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 276.5/287.3 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 287.3/287.3 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "   ---------------------------------------- 0.0/388.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 388.9/388.9 kB 12.2 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "   ---------------------------------------- 0.0/172.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 172.0/172.0 kB 10.8 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, accelerate\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
>>>>>>> parent of 8bf89e0 (scentiment analysis and some code for stock prediction)
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 269199/269199 [5:52:34<00:00, 12.73it/s]  \n"
=======
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'c:\\\\Python311\\\\Scripts\\\\huggingface-cli.exe' -> 'c:\\\\Python311\\\\Scripts\\\\huggingface-cli.exe.deleteme'\n",
      "\n"
>>>>>>> parent of 8bf89e0 (scentiment analysis and some code for stock prediction)
     ]
    }
   ],
   "source": [
    "%pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# import required libraries\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaTokenizer, RobertaForSequenceClassification\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Transformer'"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from Transformer import RobertaTokenizer, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model and evaluating\n",
    "\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scentiment\n",
       "1    196795\n",
       "2     46757\n",
       "0     25647\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('analysed_tweets.csv').loc[:,'scentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stock prediction using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "\n",
    "stock_data = pd.read_csv('stock_data/NVDA_01-01-03-31')\n",
    "\n",
    "train_split = int(math.ceil(len(stock_data)*0.8))\n",
    "test_split = len(stock_data) - train_split\n",
    "\n",
    "train_data, test_data = stock_data[:train_split].iloc[:,:1], stock_data[train_split:].iloc[:,:1]\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building model and evaluating\n",
    "\n",
    "model = nn.LSTM(pretrained = True)\n",
    "\n",
    "optimizer_fn = optim.SGD(model.parameters(),lr = 0.01)\n",
    "loss_fn = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
